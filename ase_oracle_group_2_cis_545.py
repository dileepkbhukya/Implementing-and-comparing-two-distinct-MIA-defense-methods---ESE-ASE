# -*- coding: utf-8 -*-
"""ASE Oracle Group 2 CIS 545.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vuv8TlrPzS1E-ml6NlutV1BkEwpxzgc3
"""

#panda append is no longer supported in some version of python, we recommend running our files in google colab
!pip install adversarial-robustness-toolbox
!pip hash
!pip install datasketch

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn.metrics
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import statsmodels.api as sm
from sklearn.ensemble import VotingClassifier
import random
import hashlib
from sklearn.preprocessing import MinMaxScaler
from art.attacks.inference.membership_inference import MembershipInferenceBlackBoxRuleBased
from art.estimators.classification import SklearnClassifier
import time
start_time = time.time()

# import warnings filter
from warnings import simplefilter
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)

df = pd.read_csv('/content/drive/MyDrive/CIS-545_Group2FinalProject/WorkingFiles/bank-full.csv', encoding = 'utf8', delimiter=';') #change drive location to file location
df.head()

#Convert string columns to categorical
df[['job', 'marital','education','default', 'housing', 'loan', 'contact', 'month', 'poutcome','y']] = df[['job', 'marital','education','default', 'housing', 'loan', 'contact', 'month', 'poutcome','y']].astype('category')

#Get dummies for categorical columns
df = pd.concat((df,pd.get_dummies(df[['job', 'marital','education','default', 'housing', 'loan', 'contact', 'month', 'poutcome','y']], drop_first=True)),axis=1)

#X and y dataframes
X = df.drop(columns=['job', 'marital','education','default', 'housing', 'loan', 'contact', 'month', 'poutcome','y','y_yes']) #drop categorical columns
y = df['y_yes']

# Min-Max Normalization
df_cont = X.copy()
#Remove all columns between column index 8 to 42
df_cont.drop(df_cont.iloc[:, 7:], inplace=True, axis=1)

df_disc = X.copy()
#Remove all columns between column index 8 to 42
df_disc.drop(df_disc.iloc[:, :7], inplace=True, axis=1)

df_cont = (df_cont-df_cont.min())/(df_cont.max()-df_cont.min())
X_scaled= pd.concat((df_cont, df_disc), 1)

#add constant to predictor variables
X_scaled = sm.add_constant(X_scaled)

X_attack = X_scaled.copy()

# split data into training and test sets: scikit-learn
X_train, X_test, y_train, y_test = train_test_split(X_scaled.iloc[:, 0:43], y, test_size=0.20, random_state = 42)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# Construct a logistic regression model
def logistic_regression(x,y):
  logistic_model = LogisticRegression(class_weight='balanced', solver='newton-cg', penalty='l2', max_iter=1000, random_state=42)
  logistic_model = logistic_model.fit(x,y)

  coeffs = logistic_model.coef_[0,:]     # Coefficients as an array for np.dot

  return logistic_model, coeffs

def prediction(x,coeffs):
    y_pred = np.dot(x,coeffs)            # Prediction for each X
    p_pred = 1/(1+np.exp(-y_pred))             # Probability of Y = 1

    return y_pred,p_pred

def evaluation(y,p_pred):
  from sklearn import metrics
  fpr, tpr, thresholds = metrics.roc_curve(y, p_pred);

  threshold = 0.75                                                # Default threshold = 0.5
  Y_pred = pd.DataFrame() # Declare a panda
  Y_pred['Class_true'] = y
  Y_pred['Class_pred'] = 1*(p_pred>threshold)
  Y_pred['P_pred'] = p_pred
  Y_pred['TP'] = 1*((Y_pred['Class_true']==1) & (Y_pred['Class_pred'] == 1))
  Y_pred['FP'] = 1*((Y_pred['Class_true']==0) & (Y_pred['Class_pred'] == 1))
  Y_pred['TN'] = 1*((Y_pred['Class_true']==0) & (Y_pred['Class_pred'] == 0))
  Y_pred['FN'] = 1*((Y_pred['Class_true']==1) & (Y_pred['Class_pred'] == 0))
  Y_pred

  print('#################################')
  print('Accuracy = ', metrics.accuracy_score(Y_pred['Class_true'],Y_pred['Class_pred']))
  print('#################################')
  return Y_pred

#Get Logistic Model and Summary
model,coeffs = logistic_regression(X_train,y_train)

#How good is the model with train data?
y_pred,p_pred = prediction(X_train,coeffs)
Y_pred = evaluation(y_train,p_pred)

#Test Results
y_pred,p_pred = prediction(X_test,coeffs)
Y_pred = evaluation(y_test,p_pred)

# Create a training data frame to split in n disjoint subsets
df_train = X_train.copy()
df_train['y_yes'] = y_train.copy()

#split in n disjoint subsets
n = 6
size = int((len(df_train) - len(df_train)%n)/n) #removing the remainder is discarding some data
#print(len(df_train))
sample_ = {}
df_holder = df_train

for i in range(1, n + 1):
    sample_[i] = df_holder.sample(size)
    df_holder = df_holder.drop(sample_[i].index)

#Build Models for ensemble
X_sample_={}
y_sample_={}
y_pred_={}
p_pred_={}
Y_pred_={}
model_={}
coeffs_={}


#X and y dataframes
for i in range(1,n+1,1):
  print(i)
  X_sample_[i] = sample_[i].drop(columns=['y_yes']) #drop y column
  y_sample_[i] = sample_[i]['y_yes']
  #print(X_sample_[i])
  #Get Logistic Model and Summary
  model_[i],coeffs_[i] = logistic_regression(X_sample_[i],y_sample_[i])
  #How good is the model with train data?
  y_pred_[i],p_pred_[i] = prediction(X_sample_[i],coeffs)
  Y_pred_[i] = evaluation(y_sample_[i],p_pred_[i])

#Formating the models for ensemble input
model_list = list(model_.values())
indice_list = list(np.array(list(model_.keys()), dtype=str))
combined_original=[]
for i in range (len(model_)):
  combined_original.append((indice_list[i],model_list[i]))

#Running ensemble input
ensemble_original = VotingClassifier(estimators=combined_original, voting='soft')
ensemble_original.fit(X_train,y_train)

y_pred_original = ensemble_original.predict(X_train)
p_pred_original = ensemble_original.predict_proba(X_train)[:,1]
Y_pred_original = evaluation(y_train,p_pred_original)

found = False

ensemble_={}
ensemblefit_={}

X_train_minus_={}
y_train_minus_={}

# Iterate over each model
for i in range(1,n+1,1):
  model_copy = model_.copy()
  X_sample_copy = X_sample_[i].iloc[:, 0:43].copy()
  y_sample_copy = y_sample_[i].copy()
  model_copy.pop(i)

  # Remove X_sample_copy out of X_train
  X_train_minus_[i] = pd.concat([X_train, X_sample_copy]).drop_duplicates(keep=False)

  # Remove y_sample_copy out of y_train
  y_train_minus_[i] = y_train.drop(list(y_sample_copy.index))

  model_list = list(model_copy.values())
  indice_list = list(np.array(list(model_copy.keys()), dtype=str))
  combined=[]
  for j in range (len(model_copy)):
      combined.append((indice_list[j],model_list[j]))


  #Running ensemble input
  ensemble_[i] = VotingClassifier(estimators=combined, voting='soft', n_jobs=-1)
  ensemble_[i].fit(X_train_minus_[i],y_train_minus_[i])

ensemble_[n+1] = VotingClassifier(estimators=combined_original, voting='soft')
ensemble_[n+1].fit(X_train,y_train)

def input_perturbation(X, S, epsilon, delta):
  '''
    X: training dataset
    S: a d-dimensional vector that holds sensitivity values of each feature in X_i (training example).
    epsilon: privacy budget
    delta: failure probability (delta = 0 for pure epsilon-DP)
    function returns perturbed version of X.
  '''
  # From Homework 5 before scaling the distributions were way off the original, after scaling probabilties remained close
  d=len(S)
  X_perturbed = X.copy()
  for i in range(len(X)):
    for j in range(d):
        if j < 9: #ignoring numerical columns
          X_perturbed[i][j]=X[i][j]
        else:
          upper_bound, lower_bound = S[j]
          noise = np.random.normal(loc=0, scale=(upper_bound-lower_bound) * np.sqrt(2*np.log(1.25/delta))/ (epsilon*d))
          X_perturbed[i][j] = X[i][j] + noise
          X_perturbed[i][j] = np.clip(X_perturbed[i][j], lower_bound, upper_bound)
  return X_perturbed
#Function to determine the sensitivity for each feature of X_train
def sensitivity_vector(X):
    sensitivity_vec = []
    for j in range(X.shape[1]):
        sensitivity = (np.max(X[:, j]), np.min(X[:, j]))
        sensitivity_vec.append(sensitivity)
    return sensitivity_vec

delta = 1e-5
epsilon = 0.8

X_attack_col = X_attack.columns

X_attack_vector= X_attack.values

S= sensitivity_vector(X_attack_vector)

X_attack_perturbed = input_perturbation(X_attack_vector, S, epsilon, delta)

X_attack_df=pd.DataFrame(X_attack_perturbed, columns=X_attack_col)

columns_to_round = X_attack_df.columns[8:43]

# Function to round values based on condition similarities don't seem to be generated if we hash decimals on the categorical variables
def round_values(val):
    return 0 if val < 0.5 else 1
X_attack_df[columns_to_round] = X_attack_df[columns_to_round].applymap(round_values)

from datasketch import MinHash, MinHashLSHForest

# Create MinHash objects for each data point
def generate_minhash_objects(data_points, num_perm=128, seed=54):
  minhash_objects = []
  for index, row in data_points.iterrows():
    minhash = MinHash(num_perm=128)  #
    for value in row:
        minhash.update(str(value).encode('utf8'))
    minhash_objects.append(minhash)
  return minhash_objects

columns_to_convert = X_attack_df.columns[8:]
X_attack_df_hash= X_attack_df.copy()
X_attack_df_hash[columns_to_convert] = X_attack_df_hash[columns_to_convert].astype(np.uint8)

minhash_objects_X = generate_minhash_objects(X_scaled, num_perm=128)

forest = MinHashLSHForest(num_perm=128)  # Same num_perm as used for MinHash

# Add MinHash objects to the forest
for i, minhash in enumerate(minhash_objects_X):
    forest.add(i, minhash)

# Index the forest
forest.index()

# Generate MinHash objects for the attack data
minhash_objects_sampled = generate_minhash_objects(X_attack_df_hash, num_perm=128)

#This is the Actual comparison part of the oracle we moved it out of a function to get it working
queries_indices = []  # List to store the indices of the queries and their nearest neighbors

for idx, query_minhash in enumerate(minhash_objects_sampled):
    result = forest.query(query_minhash, 25)  # Retrieve 25 approximate nearest neighbors

    min_distance = float('inf')  # Initialize minimum distance
    nearest_neighbor_idx = -1  # Initialize index of the nearest neighbor
    # Find the nearest single neighbor in the original dataset from the 25 neighbors
    for neighbor_idx in result:
        distance = 1 - query_minhash.jaccard(minhash_objects_X[neighbor_idx])
        if distance < min_distance:
            min_distance = distance
            nearest_neighbor_idx = neighbor_idx

    queries_indices.append((idx, nearest_neighbor_idx))  # Store the index of the nearest neighbor

#Approximate SIGNATURE MATCHING EXCLUSION - this is just the exclusion part of the oracle
def AESOracle(user_input,ensemble_,X_sample_):

  found = False

  # Iterate over each model
  for i in range(1, n + 1):
      numerical_indexes = X_sample_[i].index.tolist()
      for j in numerical_indexes:
        if user_input[1] == j:
                idx = X_sample_[i].index.get_loc(user_input[1])
                return i, idx  # return the model to be excluded and the index of the value in our training set to exclude
  idx = X_scaled.index.get_loc(user_input[1])
  return n + 1, idx # case where no model is to be excluded and the index of the value in our set to use

# split data into training and test sets: scikit-learn
X_attack_train, X_attack_test, y_attack_train, y_attack_test = train_test_split(queries_indices, y, test_size=0.15)

print(len(X_attack_train))
print(len(y_attack_train))
print(len(X_attack_test))
print(len(y_attack_test))

federated_data1 = []

input = X_attack_train

for row in input:
  federated_data1.append(AESOracle(row,ensemble_,X_sample_))

federated_data2 = []

input = X_attack_test

for row in input:
  federated_data2.append(AESOracle(row,ensemble_,X_sample_))

X_attack_train_= {}
X_attack_test_= {}

y_attack_train_={}
y_attack_test_={}

for i in range(1, len(ensemble_)+1):
  X_attack_train_[i] = pd.DataFrame(columns=X_attack_df.columns)
  X_attack_test_[i] = pd.DataFrame(columns=X_attack_df.columns)

  y_attack_train_[i] = pd.Series()
  y_attack_test_[i] = pd.Series()

for modelnumber, idx in federated_data1:
  X_attack_train_[modelnumber] = X_attack_train_[modelnumber].append(X_attack_df.loc[[idx]])
  y_attack_train_[modelnumber] = y_attack_train_[modelnumber].append(y.loc[[idx]])

for modelnumber, idx in federated_data2:
  X_attack_test_[modelnumber] = X_attack_test_[modelnumber].append(X_attack_df.loc[[idx]])
  y_attack_test_[modelnumber] = y_attack_test_[modelnumber].append(y.loc[[idx]])

X_train_minus_attack_={}
y_train_minus_attack_={}

X_test_minus_attack_={}
y_test_minus_attack_={}

for i in range(1,n+1,1):

  # Remove X_sample_copy_a out of X_train
  X_train_minus_attack_[i] = pd.concat([X_attack_df.iloc[:,0:43], X_attack_train_[i]]).drop_duplicates(keep=False)

  # Remove y_sample_copy_a out of y_train
  y_train_minus_attack_[i] = y.drop(list(y_attack_train_[i].index))


  # Remove X_sample_copy_a out of X_train
  X_test_minus_attack_[i] = pd.concat([X_attack_df.iloc[:,0:43], X_attack_test_[i]]).drop_duplicates(keep=False)

  # Remove y_sample_copy_a out of y_train
  y_test_minus_attack_[i] = y.drop(list(y_attack_test_[i].index))

X_train_minus_attack_[7] = pd.concat([X_attack_df.iloc[:,0:43], X_attack_train_[7]]).drop_duplicates(keep=False)
X_test_minus_attack_[7] = pd.concat([X_attack_df.iloc[:,0:43], X_attack_test_[7]]).drop_duplicates(keep=False)
y_train_minus_attack_[7] = y.drop(list(y_attack_train_[7].index))
y_test_minus_attack_[7] = y.drop(list(y_attack_test_[7].index))

from art.attacks.inference.membership_inference import MembershipInferenceBlackBoxRuleBased
from art.estimators.classification import SklearnClassifier
#model
#ensemble_original

for i in range(1, len(ensemble_)+1):
  # Step 1: Choose a classifier
  classifier = ensemble_[i]

  if i != len(ensemble_):
    # Step 2: Train the classifier
    classifier.fit(X_train_minus_[i].to_numpy(), y_train_minus_[i].to_numpy())
  else:
    classifier.fit(X_test.to_numpy(), y_test.to_numpy())

  # Step 3: Convert the scikit-learn classifier to an ART-compatible classifier
  estimator = SklearnClassifier(model=classifier)

  # Create the MembershipInferenceBlackBoxRuleBased attack
  attack = MembershipInferenceBlackBoxRuleBased(estimator)

  # Now you can proceed with the attack using the specified classifier
  import numpy as np
  from art.attacks.inference.membership_inference import MembershipInferenceBlackBoxRuleBased

  # infer attacked feature
  inferred_train = attack.infer(X_train_minus_attack_[i].to_numpy(), y_train_minus_attack_[i].to_numpy())
  inferred_test = attack.infer(X_test_minus_attack_[i].to_numpy(), y_test_minus_attack_[i].to_numpy())

  # check accuracy
  train_acc = np.sum(inferred_train) / len(inferred_train)
  test_acc = 1 - (np.sum(inferred_test) / len(inferred_test))
  acc = (train_acc * len(inferred_train) + test_acc * len(inferred_test)) / (len(inferred_train) + len(inferred_test))
  print(f"Members Accuracy: {train_acc:.4f}")
  print(f"Non Members Accuracy {test_acc:.4f}")
  print(f"Attack Accuracy {acc:.4f}")

#attack data was stored differently than the ESE oracle, to generate some graphs on the unprotected data, had to reconstruct it...
attack_train_indexes = X_attack_train
attack_test_indexes = X_attack_test

X_attack_train2 = []
X_attack_test2 = []

# For X_attack_train
for index_tuple in attack_train_indexes:
    index = int(index_tuple[1])
    row = X_attack_df.iloc[index]
    X_attack_train2.append(row)

# For X_attack_test
for index_tuple in attack_test_indexes:
    index = int(index_tuple[1])
    row = X_attack_df.iloc[index]
    X_attack_test2.append(row)

X_attack_train_df = pd.DataFrame(X_attack_train2)
X_attack_test_df = pd.DataFrame(X_attack_test2)

classifier = model
classifier.fit(X_train.to_numpy(), y_train.to_numpy())
estimator = SklearnClassifier(model=classifier)
attack = MembershipInferenceBlackBoxRuleBased(estimator)

# infer attacked feature
from art.attacks.inference.membership_inference import MembershipInferenceBlackBoxRuleBased
from art.estimators.classification import SklearnClassifier
inferred_train = attack.infer(X_attack_train_df.to_numpy(), y_attack_train.to_numpy())
inferred_test = attack.infer(X_attack_test_df.to_numpy(), y_attack_test.to_numpy())

# check accuracy
train_acc = np.sum(inferred_train) / len(inferred_train)
test_acc = 1 - (np.sum(inferred_test) / len(inferred_test))
acc = (train_acc * len(inferred_train) + test_acc * len(inferred_test)) / (len(inferred_train) + len(inferred_test))
print(f"Members Accuracy: {train_acc:.4f}")
print(f"Non Members Accuracy {test_acc:.4f}")
print(f"Attack Accuracy {acc:.4f}")

print("--- %s seconds ---" % (time.time() - start_time))